{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08 - External Index Retrievers üåê\n",
    "\n",
    "## Learning Objectives üéØ\n",
    "\n",
    "In this notebook, you'll learn:\n",
    "\n",
    "1. **What are External Index Retrievers** and how they differ from vector store retrievers\n",
    "2. **ArxivRetriever** - Search and retrieve scholarly articles from arxiv.org\n",
    "3. **WikipediaRetriever** - Access Wikipedia articles for general knowledge\n",
    "4. **TavilySearchAPIRetriever** - Perform real-time internet searches\n",
    "5. **Integration with RAG Chains** - Combine external retrievers with LLMs\n",
    "6. **Best Practices** - When and how to use each retriever effectively\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents üìö\n",
    "\n",
    "1. [Introduction to External Retrievers](#intro)\n",
    "2. [Setup & Installation](#setup)\n",
    "3. [ArxivRetriever - Academic Papers](#arxiv)\n",
    "4. [WikipediaRetriever - General Knowledge](#wikipedia)\n",
    "5. [TavilySearchAPIRetriever - Web Search](#tavily)\n",
    "6. [Integration with RAG Chains](#rag)\n",
    "7. [Comparison & Use Cases](#comparison)\n",
    "8. [Best Practices](#best-practices)\n",
    "9. [Summary & Exercises](#summary)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## 1. Introduction to External Index Retrievers üîç\n",
    "\n",
    "### What are External Index Retrievers?\n",
    "\n",
    "**External Index Retrievers** search over external data sources (e.g., the internet, academic databases, knowledge bases) rather than your local vector store.\n",
    "\n",
    "### Key Differences:\n",
    "\n",
    "| Feature | Vector Store Retrievers | External Index Retrievers |\n",
    "|---------|------------------------|---------------------------|\n",
    "| **Data Source** | Your embedded documents | External databases/APIs |\n",
    "| **Data Freshness** | Static (at indexing time) | Real-time or regularly updated |\n",
    "| **Setup Required** | Embedding + Vector store | API keys (sometimes) |\n",
    "| **Use Cases** | Internal documents, knowledge bases | Current events, academic research, general knowledge |\n",
    "| **Cost** | Embedding cost + storage | API calls (often free tier available) |\n",
    "\n",
    "### When to Use External Retrievers:\n",
    "\n",
    "- ‚úÖ You need **up-to-date information** from the internet\n",
    "- ‚úÖ You want to access **specialized databases** (e.g., academic papers)\n",
    "- ‚úÖ You need **general knowledge** without building a custom knowledge base\n",
    "- ‚úÖ You want to **augment** your local data with external sources\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup'></a>\n",
    "## 2. Setup & Installation ‚öôÔ∏è\n",
    "\n",
    "### Required Packages\n",
    "\n",
    "All external retrievers are part of `langchain-community`. You'll also need:\n",
    "\n",
    "```bash\n",
    "pip install langchain-community\n",
    "pip install arxiv           # For ArxivRetriever\n",
    "pip install wikipedia       # For WikipediaRetriever\n",
    "pip install tavily-python   # For TavilySearchAPIRetriever\n",
    "```\n",
    "\n",
    "### Environment Variables\n",
    "\n",
    "For TavilySearchAPIRetriever, you'll need an API key:\n",
    "\n",
    "```\n",
    "TAVILY_API_KEY=your_api_key_here\n",
    "```\n",
    "\n",
    "Get your free API key at: https://tavily.com/\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LangChain version: 1.0.5\n",
      "‚úÖ Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Setup: Import required libraries\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Import LangChain components\n",
    "from langchain_community.retrievers import ArxivRetriever, WikipediaRetriever, TavilySearchAPIRetriever\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Verify versions\n",
    "import langchain\n",
    "print(f\"‚úÖ LangChain version: {langchain.__version__}\")\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='arxiv'></a>\n",
    "## 3. ArxivRetriever - Academic Papers üìÑ\n",
    "\n",
    "### üî∞ BEGINNER: What is ArxivRetriever?\n",
    "\n",
    "**ArxivRetriever** searches [arxiv.org](https://arxiv.org), a repository of electronic preprints for research papers in:\n",
    "- Physics\n",
    "- Mathematics\n",
    "- Computer Science\n",
    "- Quantitative Biology\n",
    "- Quantitative Finance\n",
    "- Statistics\n",
    "\n",
    "### Use Cases:\n",
    "- üìö Literature review for research\n",
    "- üß† Getting latest research on AI/ML topics\n",
    "- üìä Finding papers by specific authors\n",
    "- üî¨ Accessing cutting-edge research\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !uv pip -q install arxiv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî∞ BEGINNER: Basic ArxivRetriever Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m8 packages\u001b[0m \u001b[2min 915ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m2 packages\u001b[0m \u001b[2min 41ms\u001b[0m\u001b[0m                                              \n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m3 packages\u001b[0m \u001b[2min 3ms\u001b[0m\u001b[0m                                 \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1marxiv\u001b[0m\u001b[2m==2.3.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfeedparser\u001b[0m\u001b[2m==6.0.12\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msgmllib3k\u001b[0m\u001b[2m==1.0.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Found 3 papers on 'large language models'\n",
      "\n",
      "================================================================================\n",
      "Title: Large Language Models Lack Understanding of Character Composition of Words\n",
      "Authors: Andrew Shin, Kunitake Kaneko\n",
      "Published: 2024-07-23\n",
      "\n",
      "Abstract (first 500 chars):\n",
      "Large language models (LLMs) have demonstrated remarkable performances on a wide range of natural language tasks. Yet, LLMs' successes have been largely restricted to tasks concerning words, sentences, or documents, and it remains questionable how much they understand the minimal units of text, namely characters. In this paper, we examine contemporary LLMs regarding their ability to understand character composition of words, and show that most of them fail to reliably carry out even the simple t...\n",
      "================================================================================\n",
      "Title: Is Self-knowledge and Action Consistent or Not: Investigating Large Language Model's Personality\n",
      "================================================================================\n",
      "Title: Unmasking the Shadows of AI: Investigating Deceptive Capabilities in Large Language Models\n"
     ]
    }
   ],
   "source": [
    "# Create an ArxivRetriever instance\n",
    "# By default, it returns top 3 documents\n",
    "arxiv_retriever = ArxivRetriever(load_max_docs=3)\n",
    "\n",
    "# Search for papers on \"large language models\"\n",
    "query = \"large language models\"\n",
    "docs = arxiv_retriever.invoke(query)\n",
    "\n",
    "print(f\"üìö Found {len(docs)} papers on '{query}'\\n\")\n",
    "\n",
    "# Display first paper\n",
    "print(\"=\" * 80)\n",
    "print(f\"Title: {docs[0].metadata.get('Title', 'N/A')}\")\n",
    "print(f\"Authors: {docs[0].metadata.get('Authors', 'N/A')}\")\n",
    "print(f\"Published: {docs[0].metadata.get('Published', 'N/A')}\")\n",
    "print(f\"\\nAbstract (first 500 chars):\\n{docs[0].page_content[:500]}...\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Title: {docs[1].metadata.get('Title', 'N/A')}\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Title: {docs[2].metadata.get('Title', 'N/A')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Advanced ArxivRetriever Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Retrieved 3 papers\n",
      "\n",
      "1. Quantum Computing: Vision and Challenges\n",
      "   Authors: Sukhpal Singh Gill, Oktay Cetinkaya, Stefano Marrone, Daniel Claudino, David Haunschild, Leon Schlote, Huaming Wu, Carlo Ottaviani, Xiaoyuan Liu, Sree Pragna Machupalli, Kamalpreet Kaur, Priyansh Arora, Ji Liu, Ahmed Farouk, Houbing Herbert Song, Steve Uhlig, Kotagiri Ramamohanarao\n",
      "   Published: 2025-04-07\n",
      "   Entry ID: N/A\n",
      "\n",
      "2. Tierkreis: A Dataflow Framework for Hybrid Quantum-Classical Computing\n",
      "   Authors: Seyon Sivarajah, Lukas Heidemann, Alan Lawrence, Ross Duncan\n",
      "   Published: 2022-11-04\n",
      "   Entry ID: N/A\n",
      "\n",
      "3. This paper has been withdrawn\n",
      "   Authors: This paper has been withdrawn\n",
      "   Published: 2006-01-22\n",
      "   Entry ID: N/A\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Advanced: Retrieve more documents and explore metadata\n",
    "arxiv_retriever_advanced = ArxivRetriever(\n",
    "    load_max_docs=5,  # Get top 5 papers\n",
    "    load_all_available_meta=True  # Load all metadata\n",
    ")\n",
    "\n",
    "# Search for papers on \"transformers attention mechanism\"\n",
    "query = \"quantum computing optimization research paper 2025\"\n",
    "docs = arxiv_retriever_advanced.invoke(query)\n",
    "\n",
    "print(f\"üìö Retrieved {len(docs)} papers\\n\")\n",
    "\n",
    "# Display metadata for all papers\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    print(f\"{i}. {doc.metadata.get('Title', 'N/A')}\")\n",
    "    print(f\"   Authors: {doc.metadata.get('Authors', 'N/A')}\")\n",
    "    print(f\"   Published: {doc.metadata.get('Published', 'N/A')}\")\n",
    "    print(f\"   Entry ID: {doc.metadata.get('entry_id', 'N/A')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Retrieved 3 papers\n",
      "\n",
      "1. Vision Transformer with Quadrangle Attention\n",
      "   Authors: Qiming Zhang, Jing Zhang, Yufei Xu, Dacheng Tao\n",
      "   Published: 2023-03-27\n",
      "   Entry ID: N/A\n",
      "\n",
      "2. D√©j√† vu: A Contextualized Temporal Attention Mechanism for Sequential Recommendation\n",
      "   Authors: Jibang Wu, Renqin Cai, Hongning Wang\n",
      "   Published: 2020-01-29\n",
      "   Entry ID: N/A\n",
      "\n",
      "3. Self-Attention as Distributional Projection: A Unified Interpretation of Transformer Architecture\n",
      "   Authors: Nihal Mehta\n",
      "   Published: 2025-11-16\n",
      "   Entry ID: N/A\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Advanced: Retrieve more documents and explore metadata\n",
    "arxiv_retriever_advanced = ArxivRetriever(\n",
    "    load_max_docs=5,  # Get top 5 papers\n",
    "    load_all_available_meta=True  # Load all metadata\n",
    ")\n",
    "\n",
    "# Search for papers on \"transformers attention mechanism\"\n",
    "query = \"transformers attention mechanism\"\n",
    "docs = arxiv_retriever_advanced.invoke(query)\n",
    "\n",
    "print(f\"üìö Retrieved {len(docs)} papers\\n\")\n",
    "\n",
    "# Display metadata for all papers\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    print(f\"{i}. {doc.metadata.get('Title', 'N/A')}\")\n",
    "    print(f\"   Authors: {doc.metadata.get('Authors', 'N/A')}\")\n",
    "    print(f\"   Published: {doc.metadata.get('Published', 'N/A')}\")\n",
    "    print(f\"   Entry ID: {doc.metadata.get('entry_id', 'N/A')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Using .batch() for Multiple Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Batch Search Results:\n",
      "\n",
      "Query: 'RAG retrieval augmented generation'\n",
      "  ‚Üí Found 3 papers\n",
      "  ‚Üí Top result: AR-RAG: Autoregressive Retrieval Augmentation for Image Generation\n",
      "\n",
      "Query: 'vector embeddings'\n",
      "  ‚Üí Found 3 papers\n",
      "  ‚Üí Top result: Part-of-Speech Relevance Weights for Learning Word Embeddings\n",
      "\n",
      "Query: 'prompt engineering'\n",
      "  ‚Üí Found 3 papers\n",
      "  ‚Üí Top result: Towards Goal-oriented Prompt Engineering for Large Language Models: A Survey\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Batch processing: Search multiple topics at once\n",
    "queries = [\n",
    "    \"RAG retrieval augmented generation\",\n",
    "    \"vector embeddings\",\n",
    "    \"prompt engineering\"\n",
    "]\n",
    "\n",
    "arxiv_retriever_batch = ArxivRetriever(load_max_docs=3)\n",
    "batch_results = arxiv_retriever_batch.batch(queries)\n",
    "\n",
    "print(\"üìö Batch Search Results:\\n\")\n",
    "for query, docs in zip(queries, batch_results):\n",
    "    print(f\"Query: '{query}'\")\n",
    "    print(f\"  ‚Üí Found {len(docs)} papers\")\n",
    "    if docs:\n",
    "        print(f\"  ‚Üí Top result: {docs[0].metadata.get('Title', 'N/A')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Understanding ArxivRetriever Metadata\n",
    "\n",
    "Each document returned by ArxivRetriever contains rich metadata:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'Published': '2023-06-15',           # Publication date\n",
    "    'Title': 'Paper Title',              # Full title\n",
    "    'Authors': 'Author1, Author2',       # Comma-separated authors\n",
    "    'Summary': 'Abstract text...',       # Paper abstract/summary\n",
    "    'entry_id': 'http://arxiv.org/...',  # Arxiv URL\n",
    "}\n",
    "```\n",
    "\n",
    "The `page_content` field contains the full abstract/summary of the paper.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='wikipedia'></a>\n",
    "## 4. WikipediaRetriever - General Knowledge üìñ\n",
    "\n",
    "### üî∞ BEGINNER: What is WikipediaRetriever?\n",
    "\n",
    "**WikipediaRetriever** searches and retrieves content from Wikipedia, the free encyclopedia with 6+ million articles.\n",
    "\n",
    "### Use Cases:\n",
    "- üåç General knowledge questions\n",
    "- üìö Quick facts and definitions\n",
    "- üèõÔ∏è Historical information\n",
    "- üßë‚Äçüî¨ Biographical data\n",
    "- üó∫Ô∏è Geographic information\n",
    "\n",
    "### Important Notes:\n",
    "- ‚ö†Ô∏è Wikipedia content is **community-edited** - verify critical information\n",
    "- ‚úÖ Great for general knowledge, not for specialized or proprietary data\n",
    "- üåê Supports multiple languages\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî∞ BEGINNER: Basic WikipediaRetriever Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m9 packages\u001b[0m \u001b[2min 1.10s\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[2K   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m wikipedia\u001b[2m==1.4.0\u001b[0m                                           \n",
      "\u001b[2K\u001b[1A   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m wikipedia\u001b[2m==1.4.0\u001b[0m                                   \u001b[1A\n",
      "\u001b[2K\u001b[1A      \u001b[32m\u001b[1mBuilt\u001b[0m\u001b[39m wikipedia\u001b[2m==1.4.0\u001b[0m                                   \u001b[1A\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 285ms\u001b[0m\u001b[0m                                              \n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m                                  \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwikipedia\u001b[0m\u001b[2m==1.4.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Found 2 Wikipedia articles on 'Python programming language'\n",
      "\n",
      "================================================================================\n",
      "Title: Python (programming language)\n",
      "Source: https://en.wikipedia.org/wiki/Python_(programming_language)\n",
      "\n",
      "Content (first 600 chars):\n",
      "Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation. Python is dynamically type-checked and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming.\n",
      "Guido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language. Python 3.0, released in 2008, was a major revision and not completely backward-compatible with earlier versions. Beginning with Python 3.5, capabi...\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create a WikipediaRetriever instance\n",
    "# By default, it returns top 3 documents\n",
    "wiki_retriever = WikipediaRetriever(top_k_results=2)\n",
    "\n",
    "# Search for information on \"Python programming language\"\n",
    "query = \"Python programming language\"\n",
    "docs = wiki_retriever.invoke(query)\n",
    "\n",
    "print(f\"üìñ Found {len(docs)} Wikipedia articles on '{query}'\\n\")\n",
    "\n",
    "# Display first result\n",
    "print(\"=\" * 80)\n",
    "print(f\"Title: {docs[0].metadata.get('title', 'N/A')}\")\n",
    "print(f\"Source: {docs[0].metadata.get('source', 'N/A')}\")\n",
    "print(f\"\\nContent (first 600 chars):\\n{docs[0].page_content[:600]}...\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Advanced WikipediaRetriever Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Retrieved 3 Wikipedia articles\n",
      "\n",
      "1. Title: Machine learning\n",
      "   Summary: Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn...\n",
      "   Content length: 1000 characters\n",
      "\n",
      "2. Title: Neural network (machine learning)\n",
      "   Summary: In machine learning, a neural network or neural net (NN), also called artificial neural network (ANN), is a computational model inspired by the struct...\n",
      "   Content length: 1000 characters\n",
      "\n",
      "3. Title: Attention (machine learning)\n",
      "   Summary: In machine learning, attention is a method that determines the importance of each component in a sequence relative to the other components in that seq...\n",
      "   Content length: 1000 characters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Advanced: Control number of results and document length\n",
    "wiki_retriever_advanced = WikipediaRetriever(\n",
    "    top_k_results=3,        # Get top 3 results\n",
    "    doc_content_chars_max=1000  # Limit content to 1000 characters per doc\n",
    ")\n",
    "\n",
    "# Search for \"Machine Learning\"\n",
    "query = \"Machine Learning\"\n",
    "docs = wiki_retriever_advanced.invoke(query)\n",
    "\n",
    "print(f\"üìñ Retrieved {len(docs)} Wikipedia articles\\n\")\n",
    "\n",
    "# Display all results\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    print(f\"{i}. Title: {doc.metadata.get('title', 'N/A')}\")\n",
    "    print(f\"   Summary: {doc.metadata.get('summary', 'N/A')[:150]}...\")\n",
    "    print(f\"   Content length: {len(doc.page_content)} characters\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Multilingual Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê Search in Spanish Wikipedia: 'Inteligencia Artificial'\n",
      "\n",
      "Title: Inteligencia artificial\n",
      "Content preview:\n",
      "La inteligencia artificial, abreviado como IA, en el contexto de las ciencias de la computaci√≥n, es una disciplina y un conjunto de capacidades cognoscitivas e intelectuales expresadas por sistemas inform√°ticos o combinaciones de algoritmos cuyo prop√≥sito es la creaci√≥n de m√°quinas que imiten la inteligencia humana.\n",
      "Estas tecnolog√≠as permiten que las m√°quinas aprendan de la experiencia, se adapten...\n"
     ]
    }
   ],
   "source": [
    "# Search in different languages\n",
    "# Default is English ('en'), but you can specify other languages\n",
    "\n",
    "# Example: Search in Spanish\n",
    "wiki_retriever_es = WikipediaRetriever(\n",
    "    top_k_results=1,\n",
    "    lang=\"es\"  # Spanish Wikipedia\n",
    ")\n",
    "\n",
    "query = \"Inteligencia Artificial\"\n",
    "docs = wiki_retriever_es.invoke(query)\n",
    "\n",
    "print(f\"üåê Search in Spanish Wikipedia: '{query}'\\n\")\n",
    "print(f\"Title: {docs[0].metadata.get('title', 'N/A')}\")\n",
    "print(f\"Content preview:\\n{docs[0].page_content[:400]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Batch Processing with WikipediaRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìñ Batch Wikipedia Search Results:\n",
      "\n",
      "Query: 'Albert Einstein'\n",
      "  ‚Üí Title: Albert Einstein\n",
      "  ‚Üí Summary: Albert Einstein (14 March 1879 ‚Äì 18 April 1955) was a German-born theoretical physicist best known for developing the theory of relativity. Einstein also made important contributions to quantum theory...\n",
      "\n",
      "Query: 'Quantum Computing'\n",
      "  ‚Üí Title: Quantum computing\n",
      "  ‚Üí Summary: A quantum computer is a (real or theoretical) computer that exploits superposed and entangled states, and the intrinsically non-deterministic outcomes of quantum measurements, as features of its compu...\n",
      "\n",
      "Query: 'Neural Networks'\n",
      "  ‚Üí Title: Neural network (machine learning)\n",
      "  ‚Üí Summary: In machine learning, a neural network or neural net (NN), also called artificial neural network (ANN), is a computational model inspired by the structure and functions of biological neural networks.\n",
      "A...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Batch search for multiple topics\n",
    "queries = [\n",
    "    \"Albert Einstein\",\n",
    "    \"Quantum Computing\",\n",
    "    \"Neural Networks\"\n",
    "]\n",
    "\n",
    "wiki_retriever_batch = WikipediaRetriever(top_k_results=1, doc_content_chars_max=500)\n",
    "batch_results = wiki_retriever_batch.batch(queries)\n",
    "\n",
    "print(\"üìñ Batch Wikipedia Search Results:\\n\")\n",
    "for query, docs in zip(queries, batch_results):\n",
    "    print(f\"Query: '{query}'\")\n",
    "    if docs:\n",
    "        print(f\"  ‚Üí Title: {docs[0].metadata.get('title', 'N/A')}\")\n",
    "        print(f\"  ‚Üí Summary: {docs[0].page_content[:200]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Understanding WikipediaRetriever Metadata\n",
    "\n",
    "Each document returned by WikipediaRetriever contains:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'title': 'Article Title',           # Wikipedia article title\n",
    "    'summary': 'Brief summary...',       # Short summary (if available)\n",
    "    'source': 'https://en.wikipedia...', # Full Wikipedia URL\n",
    "}\n",
    "```\n",
    "\n",
    "The `page_content` field contains the article text (up to `doc_content_chars_max` characters).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tavily'></a>\n",
    "## 5. TavilySearchAPIRetriever - Web Search üîç\n",
    "\n",
    "### üî∞ BEGINNER: What is TavilySearchAPIRetriever?\n",
    "\n",
    "**TavilySearchAPIRetriever** performs **real-time internet searches** using the Tavily Search API, optimized for AI applications.\n",
    "\n",
    "### Key Features:\n",
    "- üåê **Real-time web search** - Get the latest information from the internet\n",
    "- üéØ **AI-optimized** - Returns clean, relevant content for LLMs\n",
    "- üîí **Source attribution** - Includes URLs and metadata\n",
    "- ‚ö° **Fast & reliable** - Built specifically for AI use cases\n",
    "\n",
    "### Use Cases:\n",
    "- üì∞ Current events and news\n",
    "- üíπ Stock prices and market data\n",
    "- üå¶Ô∏è Weather information\n",
    "- üè¢ Company information\n",
    "- üîß Technical documentation and tutorials\n",
    "\n",
    "### Getting Started:\n",
    "1. Sign up at https://tavily.com/ (free tier available)\n",
    "2. Get your API key\n",
    "3. Add to `.env` file: `TAVILY_API_KEY=your_api_key`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî∞ BEGINNER: Basic TavilySearchAPIRetriever Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m14 packages\u001b[0m \u001b[2min 688ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[37m‚†ô\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)                                                   \n",
      "\u001b[2K\u001b[1A\u001b[37m‚†ô\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)-------------------\u001b[0m\u001b[0m     0 B/15.12 KiB           \u001b[1A\n",
      "\u001b[2K\u001b[1A\u001b[37m‚†ô\u001b[0m \u001b[2mPreparing packages...\u001b[0m (0/1)----------\u001b[30m\u001b[2m\u001b[0m\u001b[0m 15.12 KiB/15.12 KiB         \u001b[1A\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 47ms\u001b[0m\u001b[0m                                                        \u001b[1A\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 5ms\u001b[0m\u001b[0m13                                \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtavily-python\u001b[0m\u001b[2m==0.7.13\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Found 3 web results for 'latest developments in artificial intelligence 2024'\n",
      "\n",
      "================================================================================\n",
      "Source: https://cdss.berkeley.edu/news/what-experts-are-watching-2024-related-artificial-intelligence\n",
      "\n",
      "Content (first 500 chars):\n",
      "In 2024, we will see continued rise in AI capabilities across different domains including video generation, agents, etc. At the same time, we...\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Create a TavilySearchAPIRetriever instance\n",
    "# Make sure TAVILY_API_KEY is set in your .env file\n",
    "\n",
    "tavily_retriever = TavilySearchAPIRetriever(k=3)  # Return top 3 results\n",
    "\n",
    "# Search for \"latest developments in artificial intelligence 2024\"\n",
    "query = \"latest developments in artificial intelligence 2024\"\n",
    "docs = tavily_retriever.invoke(query)\n",
    "\n",
    "print(f\"üîç Found {len(docs)} web results for '{query}'\\n\")\n",
    "\n",
    "# Display first result\n",
    "print(\"=\" * 80)\n",
    "print(f\"Source: {docs[0].metadata.get('source', 'N/A')}\")\n",
    "print(f\"\\nContent (first 500 chars):\\n{docs[0].page_content[:500]}...\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Advanced TavilySearchAPIRetriever Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Retrieved 5 web results\n",
      "\n",
      "1. Source: https://www.elastic.co/blog/langchain-tutorial\n",
      "   Content preview: LangChain even provides standard interfaces for a few of their main modules, including memory modules (a reusable building block that stores and manages data for use by large language models) and agen...\n",
      "\n",
      "2. Source: https://www.datacamp.com/tutorial/how-to-build-llm-applications-with-langchain\n",
      "   Content preview: This tutorial is perfect for you. Here, we explore LangChain - An open-source Python framework for building applications based on Large Language Models such as...\n",
      "\n",
      "3. Source: https://medium.com/data-science-in-your-pocket/langchain-tutorials-for-newbies-945319df04e2\n",
      "   Content preview: ## LangChain in your Pocket: Beginner's Guide to Building Generative AI Applications using LLMs ### LangChain in your Pocket: Beginner's Guide to Building Generative AI Applications using LLMs eBook :...\n",
      "\n",
      "4. Source: https://docs.langchain.com/oss/python/learn\n",
      "   Content preview: [LangChain](/oss/python/langchain/overview)[LangGraph](/oss/python/langgraph/overview)[Deep Agents](/oss/python/deepagents/overview)[Integrations](/oss/python/integrations/providers/overview)[Learn](/...\n",
      "\n",
      "5. Source: https://github.com/gkamradt/langchain-tutorials\n",
      "   Content preview: 2. LangChain CookBook Part 2: 9 Use Cases - Code, Video | Kor | Eugene Yurtsev | üêí Intermediate | ‚úÖ Code | This is a half-baked prototype that ‚Äúhelps‚Äù you extract structured data from text using large...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Advanced: Control search depth and domain filtering\n",
    "from langchain_community.retrievers import TavilySearchAPIRetriever\n",
    "\n",
    "# Advanced configuration\n",
    "tavily_retriever_advanced = TavilySearchAPIRetriever(\n",
    "    k=5,  # Return top 5 results\n",
    "    # search_depth=\"advanced\",  # \"basic\" or \"advanced\" (more thorough)\n",
    "    # include_domains=[\"github.com\", \"stackoverflow.com\"],  # Filter to specific domains\n",
    "    # exclude_domains=[\"example.com\"]  # Exclude specific domains\n",
    ")\n",
    "\n",
    "# Search for \"LangChain tutorials\"\n",
    "query = \"LangChain tutorials\"\n",
    "docs = tavily_retriever_advanced.invoke(query)\n",
    "\n",
    "print(f\"üîç Retrieved {len(docs)} web results\\n\")\n",
    "\n",
    "# Display all results with sources\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    print(f\"{i}. Source: {doc.metadata.get('source', 'N/A')}\")\n",
    "    print(f\"   Content preview: {doc.page_content[:200]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Real-Time Information Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üïê Real-Time Information (as of November 26, 2025):\n",
      "\n",
      "Query: 'latest AI news November 26, 2025'\n",
      "  ‚Üí This November 25-26, 2025, the iconic Grand Palais will host the business-focused continuation of the AI Action Summit, led by President Macron alongside...\n",
      "  ‚Üí Source: https://www.himss.org/events-overview/adopt-ai-2025/\n",
      "\n",
      "Query: 'current weather in San Francisco'\n",
      "  ‚Üí {'location': {'name': 'San Francisco', 'region': 'California', 'country': 'United States of America', 'lat': 37.775, 'lon': -122.4183, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1764100001, 'localtime': '2025-11-25 11:46'}, 'current': {'last_...\n",
      "  ‚Üí Source: https://www.weatherapi.com/\n",
      "\n",
      "Query: 'NVIDIA stock price today'\n",
      "  ‚Üí 1 day ago ¬∑ Get the latest NVIDIA Corporation NVDA detailed stock quotes, stock data, Real-Time ECN, charts, stats and more....\n",
      "  ‚Üí Source: https://www.zacks.com/stock/quote/NVDA\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: Get current information (news, weather, stock prices, etc.)\n",
    "from datetime import datetime\n",
    "\n",
    "current_date = datetime.now().strftime(\"%B %d, %Y\")\n",
    "\n",
    "# Real-time queries\n",
    "queries = [\n",
    "    f\"latest AI news {current_date}\",\n",
    "    \"current weather in San Francisco\",\n",
    "    \"NVIDIA stock price today\"\n",
    "]\n",
    "\n",
    "tavily_realtime = TavilySearchAPIRetriever(k=2)\n",
    "\n",
    "print(f\"üïê Real-Time Information (as of {current_date}):\\n\")\n",
    "\n",
    "for query in queries:\n",
    "    docs = tavily_realtime.invoke(query)\n",
    "    print(f\"Query: '{query}'\")\n",
    "    if docs:\n",
    "        print(f\"  ‚Üí {docs[0].page_content[:250]}...\")\n",
    "        print(f\"  ‚Üí Source: {docs[0].metadata.get('source', 'N/A')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä Understanding TavilySearchAPIRetriever Metadata\n",
    "\n",
    "Each document returned by TavilySearchAPIRetriever contains:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'source': 'https://example.com/...',  # Source URL\n",
    "    'score': 0.95,                         # Relevance score (0-1)\n",
    "    'title': 'Page Title',                 # Web page title (if available)\n",
    "}\n",
    "```\n",
    "\n",
    "The `page_content` field contains the extracted text content from the web page.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rag'></a>\n",
    "## 6. Integration with RAG Chains üîó\n",
    "\n",
    "Now let's combine external retrievers with LLMs to build powerful **Retrieval-Augmented Generation (RAG)** systems!\n",
    "\n",
    "### üî∞ BEGINNER: Simple QA Chain with External Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is quantum computing and how does it work?\n",
      "\n",
      "Answer: Quantum computing is a type of computation that exploits superposed and entangled states, and the intrinsically non-deterministic outcomes of quantum measurements. It can be viewed as sampling from quantum systems that evolve in ways that may be described as operating on an enormous number of possibilities simultaneously, though still subject to strict computational constraints. Unlike classical computers that operate according to deterministic rules, quantum computers leverage quantum mechanics to perform calculations. The basic unit of information is the qubit, which can exist in a superposition of states. Quantum algorithms are designed to manipulate qubits in a way that wave interference effects amplify the probability of the desired measurement result.\n"
     ]
    }
   ],
   "source": [
    "# Build a simple RAG chain using WikipediaRetriever\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Initialize components\n",
    "wiki_retriever = WikipediaRetriever(top_k_results=2, doc_content_chars_max=2000)\n",
    "# llm = ChatGoogleGenerativeAI(model=\"gpt-5-nano\", temperature=0)\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",       # Free-tier compatible\n",
    "    temperature=0,             # Deterministic outputs\n",
    "    max_tokens=2000,\n",
    ")\n",
    "\n",
    "# Create prompt template\n",
    "template = \"\"\"Answer the question based on the following context from Wikipedia:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Helper function to format documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Build the RAG chain using LCEL\n",
    "rag_chain = (\n",
    "    {\"context\": wiki_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Ask a question\n",
    "question = \"What is quantum computing and how does it work?\"\n",
    "answer = rag_chain.invoke(question)\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Multi-Source RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are transformers in machine learning?\n",
      "\n",
      "Answer (from multiple sources):\n",
      "Transformers in machine learning are a type of artificial neural network architecture primarily based on the multi-head attention mechanism. They are a foundational element in modern AI, significantly contributing to the recent AI boom.\n",
      "\n",
      "Here's a breakdown based on the provided sources:\n",
      "\n",
      "*   **Architecture:** Transformers utilize the multi-head attention mechanism. In this architecture, text is converted into numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table. At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplified and less.\n",
      "\n",
      "*   **Origin:** The transformer architecture was introduced in the 2017 research paper \"Attention Is All You Need\" by Google researchers. This paper is considered a landmark in the field.\n",
      "\n",
      "*   **Impact:** Transformers have become the main architecture for a wide variety of AI applications, including large language models.\n",
      "\n",
      "In essence, transformers leverage attention mechanisms to process and understand data, particularly sequential data like text, making them a crucial component in many modern machine learning models.\n"
     ]
    }
   ],
   "source": [
    "# Advanced: Combine multiple retrievers for comprehensive answers\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "# Initialize multiple retrievers\n",
    "arxiv_retriever = ArxivRetriever(load_max_docs=2)\n",
    "wiki_retriever = WikipediaRetriever(top_k_results=2, doc_content_chars_max=1500)\n",
    "\n",
    "# Function to combine results from multiple retrievers\n",
    "def multi_retriever(query):\n",
    "    \"\"\"Retrieve from multiple sources and combine results.\"\"\"\n",
    "    arxiv_docs = arxiv_retriever.invoke(query)\n",
    "    wiki_docs = wiki_retriever.invoke(query)\n",
    "    \n",
    "    # Combine and format\n",
    "    all_docs = []\n",
    "    \n",
    "    if arxiv_docs:\n",
    "        all_docs.append(\"=== Academic Papers (ArXiv) ===\")\n",
    "        all_docs.extend([doc.page_content[:500] for doc in arxiv_docs])\n",
    "    \n",
    "    if wiki_docs:\n",
    "        all_docs.append(\"\\n=== General Knowledge (Wikipedia) ===\")\n",
    "        all_docs.extend([doc.page_content[:500] for doc in wiki_docs])\n",
    "    \n",
    "    return \"\\n\\n\".join(all_docs)\n",
    "\n",
    "# Create multi-source RAG chain\n",
    "multi_source_template = \"\"\"Answer the question using information from multiple sources below:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Provide a comprehensive answer that synthesizes information from both academic and general sources:\"\"\"\n",
    "\n",
    "multi_prompt = ChatPromptTemplate.from_template(multi_source_template)\n",
    "\n",
    "multi_rag_chain = (\n",
    "    {\"context\": multi_retriever, \"question\": RunnablePassthrough()}\n",
    "    | multi_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Ask a question\n",
    "question = \"What are transformers in machine learning?\"\n",
    "answer = multi_rag_chain.invoke(question)\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(f\"Answer (from multiple sources):\\n{answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéì INTERMEDIATE: Real-Time RAG with TavilySearchAPIRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the latest developments in AI regulation?\n",
      "\n",
      "Answer (from real-time web search):\n",
      "The landscape of AI regulation is rapidly evolving, with different jurisdictions adopting diverse approaches to address the opportunities and risks presented by artificial intelligence. Here's a summary of the latest developments, drawing from the provided information and expanding with broader context:\n",
      "\n",
      "**Global Trends and Diverging Philosophies:**\n",
      "\n",
      "The overarching trend is a global recognition of the need for AI governance. However, the *how* of that governance is where significant divergence emerges. Two primary philosophies are currently battling for dominance: a prescriptive, rules-based approach exemplified by the European Union, and a more flexible, risk-based approach favored by the United Kingdom and increasingly adopted by other nations.\n",
      "\n",
      "**The European Union's AI Act: A Prescriptive Approach**\n",
      "\n",
      "The EU's AI Act, now in force, represents the most comprehensive and ambitious attempt to regulate AI globally. It takes a risk-based approach, categorizing AI systems based on their potential harm.  High-risk AI systems, such as those used in critical infrastructure, education, employment, and law enforcement, are subject to stringent requirements, including:\n",
      "\n",
      "*   **Transparency and Explainability:**  Developers must provide clear information about how their AI systems work and how decisions are made.\n",
      "*   **Human Oversight:**  Mechanisms must be in place to ensure human intervention and control over AI systems.\n",
      "*   **Data Governance:**  High-quality, relevant, and unbiased data must be used to train AI systems.\n",
      "*   **Robustness and Accuracy:**  AI systems must be reliable, accurate, and resilient to errors and attacks.\n",
      "*   **Conformity Assessment:**  AI systems must undergo rigorous testing and certification before being placed on the market.\n",
      "\n",
      "The Act also prohibits certain AI practices deemed unacceptable, such as real-time biometric identification in public spaces (with limited exceptions), AI systems that manipulate human behavior, and AI systems that exploit vulnerabilities of individuals or groups.\n",
      "\n",
      "A key aspect of the AI Act is its specific rules for generative AI, which will apply from August 2025. These rules aim to address the unique risks associated with generative AI models, such as the creation of deepfakes and the spread of misinformation.  Providers of general-purpose AI models will be required to:\n",
      "\n",
      "*   Disclose that the content was AI-generated.\n",
      "*   Design the model to prevent it from generating illegal content.\n",
      "*   Publish summaries of copyrighted data used for training.\n",
      "\n",
      "The EU's approach is driven by a commitment to protecting fundamental rights, promoting ethical AI development, and fostering trust in AI technologies. However, critics argue that the Act's prescriptive nature could stifle innovation and create unnecessary bureaucratic burdens.\n",
      "\n",
      "**The United Kingdom's Flexible Approach**\n",
      "\n",
      "In contrast to the EU, the UK is pursuing a more flexible and principles-based approach to AI regulation.  The UK government believes that overly prescriptive regulations could hinder innovation and limit the potential benefits of AI. Instead, it favors a framework that allows for adaptation and evolution as AI technologies continue to develop.\n",
      "\n",
      "The UK's approach is based on five core principles:\n",
      "\n",
      "*   **Safety and Security:** AI systems should be safe and secure.\n",
      "*   **Transparency and Explainability:** AI systems should be transparent and explainable.\n",
      "*   **Fairness:** AI systems should be fair and non-discriminatory.\n",
      "*   **Accountability and Governance:**  Clear lines of accountability and governance should be established for AI systems.\n",
      "*   **Contestability and Redress:**  Mechanisms should be in place to allow individuals to challenge AI decisions and seek redress.\n",
      "\n",
      "The UK government is working with regulators across different sectors to develop guidance and standards that align with these principles. This approach aims to provide businesses with the flexibility to innovate while ensuring that AI systems are developed and used responsibly.\n",
      "\n",
      "**Other Developments Around the World**\n",
      "\n",
      "Several other countries are also actively developing AI regulations, often drawing inspiration from either the EU or the UK model.\n",
      "\n",
      "*   **United States:** The US has taken a more sector-specific approach to AI regulation, focusing on areas such as healthcare, finance, and transportation.  President Biden's Executive Order on AI aims to promote responsible AI innovation, protect Americans' rights and safety, and advance US leadership in AI. The Executive Order directs federal agencies to develop standards and guidelines for AI development and deployment, focusing on areas such as bias, privacy, and security.\n",
      "*   **Canada:** Canada is developing a comprehensive AI and Data Act (AIDA) that would regulate high-impact AI systems. The proposed legislation would require organizations to assess and mitigate risks associated with their AI systems, ensure transparency, and establish accountability mechanisms.\n",
      "*   **Chile:** As mentioned, Chile introduced draft AI legislation in May 2024 that promotes AI while ensuring human rights. The risk-based legislation also promotes self-regulation, suggesting a leaning towards the flexible approach.\n",
      "*   **China:** China has implemented regulations on specific aspects of AI, such as algorithmic recommendations and deep synthesis technologies. These regulations focus on promoting ethical AI development, protecting user rights, and preventing the spread of misinformation.\n",
      "\n",
      "**Challenges and Future Directions**\n",
      "\n",
      "The development of AI regulations faces several challenges:\n",
      "\n",
      "*   **Keeping Pace with Technological Advancements:** AI technologies are evolving rapidly, making it difficult for regulators to keep pace. Regulations must be flexible and adaptable to address new challenges and opportunities.\n",
      "*   **Balancing Innovation and Regulation:** Striking the right balance between promoting innovation and mitigating risks is crucial. Overly restrictive regulations could stifle innovation, while insufficient regulations could lead to harmful consequences.\n",
      "*   **International Cooperation:** AI is a global technology, and international cooperation is essential to ensure that regulations are consistent and effective.\n",
      "\n",
      "Looking ahead, we can expect to see continued efforts to develop and refine AI regulations around the world.  The debate between prescriptive and flexible approaches will likely continue, with different jurisdictions adopting different models based on their specific priorities and values.  Greater emphasis will likely be placed on international cooperation to address the global challenges posed by AI. Furthermore, the focus will likely shift towards more granular regulations addressing specific AI applications and use cases, rather than broad, sweeping legislation. The development of technical standards and best practices will also play a crucial role in shaping the future of AI regulation.\n"
     ]
    }
   ],
   "source": [
    "# Build a RAG chain that uses real-time web search\n",
    "tavily_retriever = TavilySearchAPIRetriever(k=3)\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Create prompt for real-time information\n",
    "realtime_template = \"\"\"Based on the latest information from the web:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Provide an up-to-date answer having atleast 500 words with source attribution:\"\"\"\n",
    "\n",
    "realtime_prompt = ChatPromptTemplate.from_template(realtime_template)\n",
    "\n",
    "# Build real-time RAG chain\n",
    "realtime_rag_chain = (\n",
    "    {\"context\": tavily_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | realtime_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Ask a current events question\n",
    "question = \"What are the latest developments in AI regulation?\"\n",
    "answer = realtime_rag_chain.invoke(question)\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(f\"Answer (from real-time web search):\\n{answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='comparison'></a>\n",
    "## 7. Comparison & Use Cases üìä\n",
    "\n",
    "### Retriever Comparison Table\n",
    "\n",
    "| Feature | ArxivRetriever | WikipediaRetriever | TavilySearchAPIRetriever |\n",
    "|---------|----------------|-------------------|-------------------------|\n",
    "| **Data Source** | Academic papers (arxiv.org) | Wikipedia articles | Real-time web search |\n",
    "| **API Key Required** | ‚ùå No | ‚ùå No | ‚úÖ Yes (free tier) |\n",
    "| **Data Freshness** | Recent research | Regularly updated | Real-time |\n",
    "| **Best For** | Academic research, ML papers | General knowledge, definitions | Current events, news |\n",
    "| **Content Type** | Research papers, abstracts | Encyclopedia articles | Web pages, news |\n",
    "| **Default Results** | 3 papers | 3 articles | 5 results |\n",
    "| **Multilingual** | ‚ùå No | ‚úÖ Yes (300+ languages) | ‚úÖ Yes |\n",
    "| **Metadata** | Title, Authors, Published date | Title, Summary, URL | Source URL, Score |\n",
    "| **Rate Limits** | Moderate | Moderate | API-dependent |\n",
    "| **Cost** | üÜì Free | üÜì Free | üÜì Free tier + paid |\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use Each Retriever\n",
    "\n",
    "#### ‚úÖ Use **ArxivRetriever** when:\n",
    "- You need peer-reviewed academic research\n",
    "- You're building an AI/ML research assistant\n",
    "- You want the latest scientific papers\n",
    "- You need citations and author information\n",
    "\n",
    "#### ‚úÖ Use **WikipediaRetriever** when:\n",
    "- You need general knowledge and definitions\n",
    "- You want historical or biographical information\n",
    "- You're building an educational chatbot\n",
    "- You need multilingual support\n",
    "- You want reliable, community-edited content\n",
    "\n",
    "#### ‚úÖ Use **TavilySearchAPIRetriever** when:\n",
    "- You need real-time, up-to-date information\n",
    "- You're answering current events questions\n",
    "- You want to search the broader internet\n",
    "- You need to filter by specific domains\n",
    "- Your use case requires the latest data\n",
    "\n",
    "---\n",
    "\n",
    "### Combining Retrievers (Hybrid Approach)\n",
    "\n",
    "For the most comprehensive RAG system:\n",
    "\n",
    "```python\n",
    "# Pseudo-code for hybrid retrieval\n",
    "if query_type == \"academic\":\n",
    "    use ArxivRetriever\n",
    "elif query_type == \"general_knowledge\":\n",
    "    use WikipediaRetriever\n",
    "elif query_type == \"current_events\":\n",
    "    use TavilySearchAPIRetriever\n",
    "else:\n",
    "    # Use multiple retrievers and combine results\n",
    "    combine(ArxivRetriever, WikipediaRetriever, TavilySearchAPIRetriever)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='best-practices'></a>\n",
    "## 8. Best Practices üí°\n",
    "\n",
    "### General Best Practices\n",
    "\n",
    "#### 1. **Handle Errors Gracefully**\n",
    "\n",
    "```python\n",
    "try:\n",
    "    docs = retriever.invoke(query)\n",
    "except Exception as e:\n",
    "    print(f\"Error retrieving documents: {e}\")\n",
    "    docs = []  # Fallback to empty list\n",
    "```\n",
    "\n",
    "#### 2. **Set Appropriate Limits**\n",
    "\n",
    "```python\n",
    "# Don't retrieve too many documents (costs, latency)\n",
    "arxiv_retriever = ArxivRetriever(load_max_docs=3)  # ‚úÖ Good\n",
    "arxiv_retriever = ArxivRetriever(load_max_docs=100)  # ‚ùå Too many\n",
    "```\n",
    "\n",
    "#### 3. **Cache Results for Repeated Queries**\n",
    "\n",
    "```python\n",
    "# Use a simple cache to avoid redundant API calls\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=100)\n",
    "def cached_search(query: str):\n",
    "    return retriever.invoke(query)\n",
    "```\n",
    "\n",
    "#### 4. **Verify Source Attribution**\n",
    "\n",
    "```python\n",
    "# Always include sources in your responses\n",
    "for doc in docs:\n",
    "    print(f\"Source: {doc.metadata.get('source', 'N/A')}\")\n",
    "```\n",
    "\n",
    "#### 5. **Combine with Vector Store Retrievers**\n",
    "\n",
    "```python\n",
    "# Use external retrievers for general knowledge\n",
    "# Use vector stores for your proprietary data\n",
    "def hybrid_retrieve(query):\n",
    "    external_docs = wiki_retriever.invoke(query)\n",
    "    internal_docs = vector_store.similarity_search(query)\n",
    "    return external_docs + internal_docs\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Retriever-Specific Best Practices\n",
    "\n",
    "#### ArxivRetriever:\n",
    "- ‚úÖ Use specific search terms (e.g., \"BERT transformers\" vs \"AI\")\n",
    "- ‚úÖ Limit results to 3-5 papers for LLM context\n",
    "- ‚úÖ Extract metadata for citations\n",
    "- ‚ùå Don't use for non-academic queries\n",
    "\n",
    "#### WikipediaRetriever:\n",
    "- ‚úÖ Use for general knowledge, not specialized topics\n",
    "- ‚úÖ Set `doc_content_chars_max` to avoid huge documents\n",
    "- ‚úÖ Verify information for critical use cases\n",
    "- ‚ùå Don't rely on Wikipedia for real-time information\n",
    "\n",
    "#### TavilySearchAPIRetriever:\n",
    "- ‚úÖ Monitor API usage (rate limits, costs)\n",
    "- ‚úÖ Use for time-sensitive queries\n",
    "- ‚úÖ Filter by domain for specific sources\n",
    "- ‚ùå Don't use for queries that don't need real-time data\n",
    "\n",
    "---\n",
    "\n",
    "### Performance Tips\n",
    "\n",
    "1. **Use `.batch()` for multiple queries**\n",
    "   ```python\n",
    "   # ‚úÖ Efficient\n",
    "   results = retriever.batch([q1, q2, q3])\n",
    "   \n",
    "   # ‚ùå Inefficient\n",
    "   results = [retriever.invoke(q) for q in [q1, q2, q3]]\n",
    "   ```\n",
    "\n",
    "2. **Limit document length for LLM context**\n",
    "   ```python\n",
    "   # Truncate long documents to fit LLM context window\n",
    "   docs = [Document(page_content=doc.page_content[:2000], metadata=doc.metadata) \n",
    "           for doc in raw_docs]\n",
    "   ```\n",
    "\n",
    "3. **Use async methods for concurrent retrieval** (if supported)\n",
    "   ```python\n",
    "   # For async-compatible retrievers\n",
    "   import asyncio\n",
    "   docs = await retriever.ainvoke(query)\n",
    "   ```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='summary'></a>\n",
    "## 9. Summary & Exercises üìù\n",
    "\n",
    "### üéØ What You Learned\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "‚úÖ **External Index Retrievers** - Search over external data sources (internet, databases)\n",
    "\n",
    "‚úÖ **ArxivRetriever** - Retrieve academic papers from arxiv.org\n",
    "   - Use cases: Research, ML papers, citations\n",
    "   - Methods: `.invoke()`, `.batch()`\n",
    "   - Metadata: Title, Authors, Published date\n",
    "\n",
    "‚úÖ **WikipediaRetriever** - Access Wikipedia articles\n",
    "   - Use cases: General knowledge, definitions, history\n",
    "   - Features: Multilingual support, customizable length\n",
    "   - Metadata: Title, Summary, Source URL\n",
    "\n",
    "‚úÖ **TavilySearchAPIRetriever** - Real-time web search\n",
    "   - Use cases: Current events, news, real-time data\n",
    "   - Features: Domain filtering, search depth control\n",
    "   - Metadata: Source URL, Relevance score\n",
    "\n",
    "‚úÖ **RAG Integration** - Combined external retrievers with LLMs\n",
    "   - Built simple QA chains\n",
    "   - Created multi-source RAG systems\n",
    "   - Implemented real-time information retrieval\n",
    "\n",
    "‚úÖ **Best Practices** - Error handling, caching, source attribution\n",
    "\n",
    "---\n",
    "\n",
    "### üí™ Practice Exercises\n",
    "\n",
    "#### Exercise 1: Academic Research Assistant (üî∞ Beginner)\n",
    "Create a RAG chain that:\n",
    "- Uses `ArxivRetriever` to find papers on \"deep learning\"\n",
    "- Extracts the top 3 paper titles and authors\n",
    "- Summarizes each paper's abstract using an LLM\n",
    "\n",
    "#### Exercise 2: Wikipedia Fact Checker (üî∞ Beginner)\n",
    "Build a system that:\n",
    "- Takes a statement as input (e.g., \"Python was created in 1991\")\n",
    "- Uses `WikipediaRetriever` to search for relevant articles\n",
    "- Uses an LLM to verify if the statement is accurate\n",
    "\n",
    "#### Exercise 3: Multi-Source News Aggregator (üéì Intermediate)\n",
    "Create a RAG chain that:\n",
    "- Uses `TavilySearchAPIRetriever` to get latest AI news\n",
    "- Uses `WikipediaRetriever` to get background on AI topics\n",
    "- Combines both sources to provide a comprehensive news summary\n",
    "\n",
    "#### Exercise 4: Hybrid Retrieval System (üéì Intermediate)\n",
    "Build a system that:\n",
    "- Classifies queries into \"academic\", \"general\", or \"current_events\"\n",
    "- Routes to the appropriate retriever based on query type\n",
    "- Returns results from the most relevant source\n",
    "\n",
    "#### Exercise 5: Multilingual Knowledge Base (üöÄ Advanced)\n",
    "Create a system that:\n",
    "- Detects the language of the user's query\n",
    "- Uses `WikipediaRetriever` with the appropriate language setting\n",
    "- Returns answers in the user's language\n",
    "\n",
    "---\n",
    "\n",
    "### üîó Next Steps\n",
    "\n",
    "- **Notebook 09**: Advanced Retrieval Techniques (Hybrid Search, Re-ranking)\n",
    "- **Notebook 10**: Production RAG Systems (Caching, Monitoring, Scaling)\n",
    "- **LangChain Documentation**: https://python.langchain.com/docs/integrations/retrievers/\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Additional Resources\n",
    "\n",
    "- **ArXiv**: https://arxiv.org/\n",
    "- **Wikipedia API**: https://www.mediawiki.org/wiki/API:Main_page\n",
    "- **Tavily API**: https://tavily.com/\n",
    "- **LangChain Retrievers**: https://python.langchain.com/docs/modules/data_connection/retrievers/\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations!** üéâ You've mastered external index retrievers in LangChain!\n",
    "\n",
    "You can now build RAG systems that access:\n",
    "- üìÑ Academic research (ArXiv)\n",
    "- üìñ General knowledge (Wikipedia)\n",
    "- üåê Real-time web data (Tavily)\n",
    "\n",
    "Keep experimenting and building amazing AI applications! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
