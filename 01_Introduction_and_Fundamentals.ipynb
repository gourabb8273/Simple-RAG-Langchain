{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“š Notebook 01: Introduction and Fundamentals\n",
    "\n",
    "**LangChain 1.0.5+ | Mixed Level Class**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "1. What LangChain is and why it's useful\n",
    "2. LangChain's modular architecture\n",
    "3. Core concepts: Documents, Chains, and LCEL\n",
    "4. How to set up your development environment\n",
    "5. The difference between LangChain and traditional ML pipelines\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“– Table of Contents\n",
    "\n",
    "1. [What is LangChain?](#what-is-langchain)\n",
    "2. [LangChain Architecture](#architecture)\n",
    "3. [Environment Setup](#setup)\n",
    "4. [Core Concepts](#core-concepts)\n",
    "5. [Quick Start Example](#quick-start)\n",
    "6. [LangChain vs Traditional Pipelines](#comparison)\n",
    "7. [Summary & Next Steps](#summary)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"what-is-langchain\"></a>\n",
    "## 1. What is LangChain? ğŸ¤”\n",
    "\n",
    "### ğŸ”° BEGINNER SECTION\n",
    "\n",
    "**LangChain** is an open-source framework that makes it easy to build applications powered by Large Language Models (LLMs) like GPT-4, Claude, or Llama.\n",
    "\n",
    "### Why LangChain?\n",
    "\n",
    "Imagine you want to build a chatbot that can:\n",
    "- Answer questions about your company's PDF documents\n",
    "- Search through your database\n",
    "- Remember previous conversations\n",
    "- Call external APIs\n",
    "\n",
    "Without LangChain, you'd need to:\n",
    "1. âœï¸ Write code to load and parse PDFs\n",
    "2. âœï¸ Convert text to embeddings\n",
    "3. âœï¸ Store embeddings in a vector database\n",
    "4. âœï¸ Implement semantic search\n",
    "5. âœï¸ Format prompts for the LLM\n",
    "6. âœï¸ Handle LLM API calls\n",
    "7. âœï¸ Manage conversation memory\n",
    "\n",
    "**With LangChain:**\n",
    "- âœ… All these components are pre-built and ready to use\n",
    "- âœ… You just connect them like LEGO blocks\n",
    "- âœ… Focus on your application logic, not infrastructure\n",
    "\n",
    "### ğŸ“ INTERMEDIATE NOTE\n",
    "\n",
    "LangChain provides:\n",
    "- **Abstractions**: Unified interfaces for different LLMs, vector stores, and tools\n",
    "- **Chains**: Composable workflows using LCEL (LangChain Expression Language)\n",
    "- **Agents**: Autonomous systems that can use tools and make decisions\n",
    "- **Memory**: Conversation history and context management\n",
    "- **Callbacks**: Monitoring, logging, and debugging hooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"architecture\"></a>\n",
    "## 2. LangChain Architecture ğŸ—ï¸\n",
    "\n",
    "### ğŸ”° BEGINNER: Visual Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    YOUR APPLICATION                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                           â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                  LANGCHAIN FRAMEWORK                    â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚  â”‚ Document â”‚  â”‚   Text   â”‚  â”‚ Embeddingsâ”‚  â”‚ Vector  â”‚ â”‚\n",
    "â”‚  â”‚ Loaders  â”‚â†’ â”‚ Splittersâ”‚â†’ â”‚  Models   â”‚â†’ â”‚ Stores  â”‚ â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚\n",
    "â”‚  â”‚Retrieversâ”‚  â”‚ Prompts  â”‚  â”‚   LLMs   â”‚               â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚\n",
    "â”‚                                                         â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "â”‚  â”‚         LCEL (LangChain Expression Language)     â”‚   â”‚\n",
    "â”‚  â”‚         Connects everything with | operator      â”‚   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                           â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚             EXTERNAL SERVICES & DATA                    â”‚\n",
    "â”‚  â€¢ OpenAI/Anthropic APIs  â€¢ Vector Databases            â”‚\n",
    "â”‚  â€¢ PDF Files              â€¢ Websites                    â”‚\n",
    "â”‚  â€¢ Databases              â€¢ APIs                        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### ğŸ“ INTERMEDIATE: Package Structure (LangChain 1.0+)\n",
    "\n",
    "LangChain is organized into several packages:\n",
    "\n",
    "| Package | Purpose | Example Imports |\n",
    "|---------|---------|------------------|\n",
    "| **langchain-core** | Core abstractions, base classes | `from langchain_core.documents import Document` |\n",
    "| **langchain-community** | Community integrations (loaders, vector stores) | `from langchain_community.document_loaders import PyPDFLoader` |\n",
    "| **langchain-openai** | OpenAI-specific integrations | `from langchain_openai import ChatOpenAI, OpenAIEmbeddings` |\n",
    "| **langchain-text-splitters** | Text splitting utilities | `from langchain_text_splitters import RecursiveCharacterTextSplitter` |\n",
    "\n",
    "**Why this matters:** In LangChain 1.0+, you import from specific packages instead of `langchain` directly. This reduces dependencies and improves modularity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "## 3. Environment Setup ğŸ› ï¸\n",
    "\n",
    "### ğŸ”° BEGINNER: Step-by-Step Setup\n",
    "\n",
    "Let's verify your environment is ready!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.6 (v3.11.6:8b6ee5ba3b, Oct  2 2023, 11:18:21) [Clang 13.0.0 (clang-1300.0.29.30)]\n",
      "âœ… Python version is compatible\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Check Python version (should be 3.9+)\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Check if Python is 3.9 or higher\n",
    "if sys.version_info >= (3, 9):\n",
    "    print(\"âœ… Python version is compatible\")\n",
    "else:\n",
    "    print(\"âŒ Please upgrade to Python 3.9 or higher\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain version: 1.0.5\n",
      "LangChain Core version: 1.0.4\n",
      "âœ… LangChain 1.0+ detected\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Import LangChain and check version\n",
    "import langchain\n",
    "from langchain_core import __version__ as core_version\n",
    "\n",
    "print(f\"LangChain version: {langchain.__version__}\")\n",
    "print(f\"LangChain Core version: {core_version}\")\n",
    "\n",
    "# We're using LangChain 1.0.5+ for this course\n",
    "if langchain.__version__ >= \"1.0\":\n",
    "    print(\"âœ… LangChain 1.0+ detected\")\n",
    "else:\n",
    "    print(\"âŒ Please upgrade: pip install --upgrade langchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OPENAI_API_KEY found\n",
      "âœ… GOOGLE_API_KEY found\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Load environment variables (API keys)\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Check if OpenAI API key is set\n",
    "# NOTE: We don't print the actual key for security!\n",
    "if os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"âœ… OPENAI_API_KEY found\")\n",
    "else:\n",
    "    print(\"âŒ OPENAI_API_KEY not found\")\n",
    "    print(\"   Create a .env file with: OPENAI_API_KEY=your-key-here\")\n",
    "\n",
    "# Check for Google API key (optional, for Google Gemini)\n",
    "if os.getenv(\"GOOGLE_API_KEY\"):\n",
    "    print(\"âœ… GOOGLE_API_KEY found\")\n",
    "else:\n",
    "    print(\"âš ï¸  GOOGLE_API_KEY not found (optional for this notebook)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ Setting Up Your .env File\n",
    "\n",
    "If you don't have a `.env` file, create one in the project root:\n",
    "\n",
    "```bash\n",
    "# .env file\n",
    "OPENAI_API_KEY=sk-proj-your-key-here\n",
    "GOOGLE_API_KEY=your-google-key-here\n",
    "```\n",
    "\n",
    "**âš ï¸ SECURITY WARNING:**\n",
    "- Never commit `.env` files to Git\n",
    "- Add `.env` to your `.gitignore` file\n",
    "- Never hardcode API keys in your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"core-concepts\"></a>\n",
    "## 4. Core Concepts ğŸ“š\n",
    "\n",
    "### 4.1 Documents ğŸ“„\n",
    "\n",
    "### ğŸ”° BEGINNER\n",
    "\n",
    "A **Document** is LangChain's way of representing a piece of text with metadata.\n",
    "\n",
    "Think of it like a note card:\n",
    "- **Front (page_content):** The actual text\n",
    "- **Back (metadata):** Information about the text (source, page number, date, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content:\n",
      "LangChain makes building LLM applications easy!\n",
      "\n",
      "Metadata:\n",
      "{'source': 'introduction.txt', 'author': 'LangChain Team', 'date': '2025-01-15'}\n",
      "\n",
      "Source: introduction.txt\n"
     ]
    }
   ],
   "source": [
    "# Creating a Document manually\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Create a simple document\n",
    "doc = Document(\n",
    "    page_content=\"LangChain makes building LLM applications easy!\",\n",
    "    metadata={\n",
    "        \"source\": \"introduction.txt\",\n",
    "        \"author\": \"LangChain Team\",\n",
    "        \"date\": \"2025-01-15\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Access the content\n",
    "print(\"Content:\")\n",
    "print(doc.page_content)\n",
    "print(\"\\nMetadata:\")\n",
    "print(doc.metadata)\n",
    "print(f\"\\nSource: {doc.metadata['source']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ INTERMEDIATE: Why Metadata Matters\n",
    "\n",
    "Metadata is crucial for:\n",
    "1. **Citation**: Showing users where information came from\n",
    "2. **Filtering**: Only search documents from specific sources\n",
    "3. **Debugging**: Tracking which documents are being retrieved\n",
    "4. **Analytics**: Understanding which sources are most useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“„ Document 1:\n",
      "   Content: Python is a high-level programming language.\n",
      "   Category: programming\n",
      "   Difficulty: beginner\n",
      "\n",
      "ğŸ“„ Document 2:\n",
      "   Content: Machine learning is a subset of artificial intelligence.\n",
      "   Category: AI\n",
      "   Difficulty: intermediate\n",
      "\n",
      "ğŸ“„ Document 3:\n",
      "   Content: RAG combines retrieval and generation for better LLM outputs.\n",
      "   Category: AI\n",
      "   Difficulty: advanced\n"
     ]
    }
   ],
   "source": [
    "# Creating multiple documents (like a mini knowledge base)\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"Python is a high-level programming language.\",\n",
    "        metadata={\"category\": \"programming\", \"difficulty\": \"beginner\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Machine learning is a subset of artificial intelligence.\",\n",
    "        metadata={\"category\": \"AI\", \"difficulty\": \"intermediate\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"RAG combines retrieval and generation for better LLM outputs.\",\n",
    "        metadata={\"category\": \"AI\", \"difficulty\": \"advanced\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "# Print all documents\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    print(f\"\\nğŸ“„ Document {i}:\")\n",
    "    print(f\"   Content: {doc.page_content}\")\n",
    "    print(f\"   Category: {doc.metadata['category']}\")\n",
    "    print(f\"   Difficulty: {doc.metadata['difficulty']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 LCEL (LangChain Expression Language) ğŸ”—\n",
    "\n",
    "### ğŸ”° BEGINNER\n",
    "\n",
    "**LCEL** is a way to connect different components using the pipe operator `|`.\n",
    "\n",
    "Think of it like a factory assembly line:\n",
    "```\n",
    "Input â†’ Component 1 â†’ Component 2 â†’ Component 3 â†’ Output\n",
    "```\n",
    "\n",
    "### Before LCEL (Old Way - Messy!):\n",
    "```python\n",
    "output = component3(component2(component1(input)))\n",
    "```\n",
    "\n",
    "### With LCEL (New Way - Clean!):\n",
    "```python\n",
    "chain = component1 | component2 | component3\n",
    "output = chain.invoke(input)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ INTERMEDIATE: LCEL Deep Dive\n",
    "\n",
    "LCEL provides:\n",
    "- **Streaming**: Stream outputs as they're generated\n",
    "- **Batch Processing**: Process multiple inputs efficiently\n",
    "- **Async Support**: Non-blocking operations\n",
    "- **Debugging**: Better error messages and logging\n",
    "- **Type Safety**: Better IDE autocomplete\n",
    "\n",
    "**Key Methods:**\n",
    "- `.invoke(input)`: Process single input\n",
    "- `.batch([input1, input2])`: Process multiple inputs\n",
    "- `.stream(input)`: Stream output tokens\n",
    "- `.ainvoke(input)`: Async version of invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 'hello langchain'\n",
      "\n",
      "Processing:\n",
      "  Step 1: uppercase â†’ HELLO LANGCHAIN\n",
      "  Step 2: add_prefix â†’ RESULT: HELLO LANGCHAIN\n",
      "  Step 3: add_emoji â†’ âœ… RESULT: HELLO LANGCHAIN\n",
      "\n",
      "Final Output: âœ… RESULT: HELLO LANGCHAIN\n"
     ]
    }
   ],
   "source": [
    "# Simple LCEL Example: String Transformation Chain\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Create simple transformation functions\n",
    "def uppercase(text: str) -> str:\n",
    "    \"\"\"Convert text to uppercase\"\"\"\n",
    "    print(f\"  Step 1: uppercase â†’ {text.upper()}\")\n",
    "    return text.upper()\n",
    "\n",
    "def add_prefix(text: str) -> str:\n",
    "    \"\"\"Add a prefix to text\"\"\"\n",
    "    result = f\"RESULT: {text}\"\n",
    "    print(f\"  Step 2: add_prefix â†’ {result}\")\n",
    "    return result\n",
    "\n",
    "def add_emoji(text: str) -> str:\n",
    "    \"\"\"Add emoji to text\"\"\"\n",
    "    result = f\"âœ… {text}\"\n",
    "    print(f\"  Step 3: add_emoji â†’ {result}\")\n",
    "    return result\n",
    "\n",
    "# Create runnables (components that can be chained)\n",
    "uppercase_runnable = RunnableLambda(uppercase)\n",
    "prefix_runnable = RunnableLambda(add_prefix)\n",
    "emoji_runnable = RunnableLambda(add_emoji)\n",
    "\n",
    "# Build the chain using LCEL\n",
    "chain = uppercase_runnable | prefix_runnable | emoji_runnable\n",
    "\n",
    "# Execute the chain\n",
    "print(\"Input: 'hello langchain'\")\n",
    "print(\"\\nProcessing:\")\n",
    "result = chain.invoke(\"hello langchain\")\n",
    "print(f\"\\nFinal Output: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Chains ğŸ”—\n",
    "\n",
    "### ğŸ”° BEGINNER\n",
    "\n",
    "A **Chain** is a sequence of operations. Common chain types:\n",
    "\n",
    "1. **Simple Chain**: Input â†’ Process â†’ Output\n",
    "2. **RAG Chain**: Query â†’ Retrieve â†’ Generate â†’ Answer\n",
    "3. **Multi-step Chain**: Input â†’ Step 1 â†’ Step 2 â†’ Step 3 â†’ Output\n",
    "\n",
    "We'll build complex chains in later notebooks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"quick-start\"></a>\n",
    "## 5. Quick Start Example ğŸš€\n",
    "\n",
    "### ğŸ”° BEGINNER: Your First LLM Call\n",
    "\n",
    "Let's make our first call to an LLM using LangChain!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is LangChain in one sentence?\n",
      "\n",
      "Answer: LangChain is a framework for developing applications powered by large language models (LLMs) by providing tools for chaining together different components like models, data sources, and prompts.\n"
     ]
    }
   ],
   "source": [
    "# Import ChatOpenAI (the LLM interface)\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# # Initialize the LLM\n",
    "# # model: Which GPT model to use\n",
    "# # temperature: 0 = deterministic, 1 = creative\n",
    "# llm = ChatOpenAI(\n",
    "#     model=\"gpt-3.5-turbo\",  # Cheaper, faster model for learning\n",
    "#     temperature=0  # Deterministic outputs for learning\n",
    "# )\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Initialize Gemini model (FREE tier supports gemini-pro)\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",       # Free-tier compatible\n",
    "    temperature=0             # Deterministic outputs\n",
    ")\n",
    "\n",
    "# Make a simple call\n",
    "response = llm.invoke(\"What is LangChain in one sentence?\")\n",
    "\n",
    "# Print the response\n",
    "print(\"Question: What is LangChain in one sentence?\")\n",
    "print(f\"\\nAnswer: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ INTERMEDIATE: Understanding the Response Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response Type: <class 'langchain_core.messages.ai.AIMessage'>\n",
      "\n",
      "Content: LangChain is a framework for developing applications powered by large language models (LLMs) by providing tools for chaining together different components like models, data sources, and prompts.\n",
      "\n",
      "Response Metadata:\n",
      "{'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}\n"
     ]
    }
   ],
   "source": [
    "# The response is an AIMessage object with metadata\n",
    "print(\"Response Type:\", type(response))\n",
    "print(\"\\nContent:\", response.content)\n",
    "print(\"\\nResponse Metadata:\")\n",
    "print(response.response_metadata)\n",
    "\n",
    "# Access specific metadata\n",
    "if 'token_usage' in response.response_metadata:\n",
    "    usage = response.response_metadata['token_usage']\n",
    "    print(f\"\\nTokens Used:\")\n",
    "    print(f\"  Prompt: {usage.get('prompt_tokens', 'N/A')}\")\n",
    "    print(f\"  Completion: {usage.get('completion_tokens', 'N/A')}\")\n",
    "    print(f\"  Total: {usage.get('total_tokens', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”° BEGINNER: Using Prompts\n",
    "\n",
    "Instead of plain strings, use **prompt templates** for better control:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Template:\n",
      "Explain {topic} in simple terms suitable for beginners.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Create a prompt template\n",
    "# {topic} is a variable we'll fill in later\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Explain {topic} in simple terms suitable for beginners.\"\n",
    ")\n",
    "\n",
    "# View the prompt structure\n",
    "print(\"Prompt Template:\")\n",
    "print(prompt.messages[0].prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Topic: MACHINE LEARNING\n",
      "============================================================\n",
      "Imagine you're teaching a dog a new trick, like \"sit.\" You don't tell the dog *exactly* how to sit, right? Instead, you show them what \"sit\" looks like, and when they do something close, you give them a treat (positive feedback).  They try different things, learn from the feedback, and eventually figure out how to sit perfectly.\n",
      "\n",
      "**Machine learning is kind of like that!**\n",
      "\n",
      "Instead of a dog, we have a **computer**. Instead of a trick, we have a **task** (like identifying cats in pictures). And instead of treats, we have **data** and **algorithms** that help the computer learn.\n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "*   **Data:** This is the \"examples\" we give the computer. For the cat example, it would be thousands of pictures, some with cats and some without. We tell the computer which pictures have cats and which don't.\n",
      "\n",
      "*   **Algorithm:** This is the \"learning method\" the computer uses. It's like the dog trying different positions to figure out \"sit.\"  There are many different algorithms, each good for different tasks.  Some common ones are:\n",
      "    *   **Decision Trees:** Like a flowchart that asks a series of questions to make a decision.\n",
      "    *   **Neural Networks:** Inspired by the human brain, these are complex systems that can learn very intricate patterns.\n",
      "    *   **Regression:** Used to predict a number, like the price of a house based on its size and location.\n",
      "\n",
      "*   **Learning:** The computer analyzes the data and tries to find patterns.  It adjusts its \"internal settings\" (like the dog adjusting its posture) to get better at the task.  It's like the computer is trying to find the best way to identify cats based on the pictures it's seen.\n",
      "\n",
      "*   **Prediction:** Once the computer has learned, we can give it new data (a new picture) and it will try to predict the answer (whether there's a cat in the picture).\n",
      "\n",
      "**In short, machine learning is about:**\n",
      "\n",
      "1.  **Feeding a computer lots of data.**\n",
      "2.  **Using an algorithm to let the computer learn patterns from that data.**\n",
      "3.  **Using those patterns to make predictions or decisions about new data.**\n",
      "\n",
      "**Why is it useful?**\n",
      "\n",
      "Machine learning is powerful because it can:\n",
      "\n",
      "*   **Automate tasks:**  Like sorting emails into spam and not spam.\n",
      "*   **Make predictions:** Like predicting customer behavior or stock prices.\n",
      "*   **Discover insights:** Like finding hidden patterns in medical data.\n",
      "\n",
      "**Think of it like this:**\n",
      "\n",
      "*   **Traditional Programming:** You write specific rules for the computer to follow.  If X, then Y.\n",
      "*   **Machine Learning:** You give the computer data and let it figure out the rules itself.\n",
      "\n",
      "Machine learning is a vast and complex field, but hopefully, this simple explanation gives you a good starting point!\n",
      "\n",
      "============================================================\n",
      "Topic: EMBEDDINGS\n",
      "============================================================\n",
      "Okay, let's explain embeddings in a way that's easy to understand, even if you're new to the concept.\n",
      "\n",
      "**Imagine you're teaching a computer about words.**\n",
      "\n",
      "Computers are good at working with numbers, but they don't inherently understand words like \"cat,\" \"dog,\" or \"happy.\"  We need a way to represent these words in a numerical format that the computer can use.\n",
      "\n",
      "**The Problem with Simple Numbers:**\n",
      "\n",
      "One simple way might be to assign each word a unique number:\n",
      "\n",
      "*   Cat = 1\n",
      "*   Dog = 2\n",
      "*   Happy = 3\n",
      "\n",
      "But this is a bad idea!  Why? Because these numbers don't tell the computer anything about the *meaning* or *relationship* between the words.  The computer doesn't know that \"cat\" and \"dog\" are both animals, or that \"happy\" is an emotion.  It just sees arbitrary numbers.\n",
      "\n",
      "**Enter Embeddings:  Turning Words into Meaningful Vectors**\n",
      "\n",
      "Embeddings are a way to represent words (or other things) as **vectors** (lists of numbers) in a multi-dimensional space.  Think of it like creating a map where words that are similar in meaning are located closer together.\n",
      "\n",
      "**Key Ideas:**\n",
      "\n",
      "1.  **Vectors:**  Instead of a single number, each word gets a list of numbers (a vector).  For example:\n",
      "\n",
      "    *   Cat = [0.2, -0.5, 0.8, 0.1, ...]  (This vector might have 100 or even 1000 numbers!)\n",
      "    *   Dog = [0.3, -0.4, 0.7, 0.2, ...]\n",
      "    *   Happy = [-0.9, 0.6, 0.0, -0.3, ...]\n",
      "\n",
      "2.  **Multi-Dimensional Space:**  Imagine a space with many dimensions (more than you can easily visualize).  Each dimension represents a different aspect of the word's meaning.  For example:\n",
      "\n",
      "    *   One dimension might represent \"animality\" (how much the word relates to animals).\n",
      "    *   Another dimension might represent \"size.\"\n",
      "    *   Another might represent \"emotion.\"\n",
      "\n",
      "3.  **Proximity = Similarity:**  The magic of embeddings is that words with similar meanings will have vectors that are close to each other in this multi-dimensional space.  So, the vectors for \"cat\" and \"dog\" will be closer together than the vectors for \"cat\" and \"happy.\"\n",
      "\n",
      "**How are Embeddings Created?**\n",
      "\n",
      "Embeddings are typically learned from large amounts of text data.  Algorithms like Word2Vec, GloVe, and FastText analyze how words are used in context and adjust the vectors so that words that appear in similar contexts end up closer together in the embedding space.\n",
      "\n",
      "**Why are Embeddings Useful?**\n",
      "\n",
      "*   **Understanding Relationships:**  Computers can now understand that \"cat\" and \"dog\" are related because their vectors are close.\n",
      "*   **Improved Accuracy:**  In tasks like text classification, sentiment analysis, and machine translation, using embeddings can significantly improve accuracy because the computer has a better understanding of the meaning of words.\n",
      "*   **Finding Similar Items:**  Embeddings can be used to find similar documents, products, or even images.\n",
      "\n",
      "**Analogy:  Colors**\n",
      "\n",
      "Think about colors.  You could assign each color a number, but that wouldn't tell you anything about how similar the colors are.  Instead, you can represent colors using three numbers: Red, Green, and Blue (RGB).  Colors that are close together in RGB space (e.g., light blue and dark blue) are visually similar.  Embeddings do something similar for words (and other things).\n",
      "\n",
      "**In Summary:**\n",
      "\n",
      "Embeddings are a way to represent words (or other items) as vectors in a multi-dimensional space, where the position of the vector reflects the meaning and relationships of the item.  This allows computers to understand the meaning of words and perform tasks more accurately.\n",
      "\n",
      "============================================================\n",
      "Topic: VECTOR DATABASES\n",
      "============================================================\n",
      "Okay, imagine you have a bunch of things, like pictures, sentences, or even sounds.  Instead of just storing them as they are, we want to represent them as a list of numbers.  Think of it like giving each thing a unique \"fingerprint\" made of numbers.  This \"fingerprint\" is called a **vector**.\n",
      "\n",
      "**What's a Vector?**\n",
      "\n",
      "A vector is simply an ordered list of numbers.  For example:\n",
      "\n",
      "*  `[1.2, 3.5, 0.8, 2.1]` is a vector with four numbers.\n",
      "*  `[0.5, 0.9]` is a vector with two numbers.\n",
      "\n",
      "The important thing is that these numbers represent some characteristics or features of the thing they're describing.  For example, if we're talking about pictures, the numbers might represent the colors, shapes, or textures in the picture.\n",
      "\n",
      "**Why use Vectors?**\n",
      "\n",
      "The magic happens because we can use these vectors to compare things.  If two things have similar vectors (meaning their numbers are close to each other), it means they are likely similar in some way.\n",
      "\n",
      "**What's a Vector Database?**\n",
      "\n",
      "A vector database is like a regular database, but instead of storing things directly, it stores these **vectors**.  It's optimized for:\n",
      "\n",
      "1.  **Storing Vectors:**  It can handle lots and lots of these number lists efficiently.\n",
      "2.  **Searching by Similarity:**  The key feature!  You can give it a new vector (a new \"fingerprint\") and ask it to find the vectors in the database that are most similar.  This is much faster than comparing the original things themselves.\n",
      "\n",
      "**Think of it like this:**\n",
      "\n",
      "Imagine you have a library with millions of books.\n",
      "\n",
      "*   **Traditional Database:**  You'd search for books by title, author, or keywords.\n",
      "*   **Vector Database:**  You could describe the *feeling* or *theme* of a book you want (as a vector), and the database would find books with similar feelings or themes, even if you don't know the title or author.\n",
      "\n",
      "**Here's a simple analogy:**\n",
      "\n",
      "Imagine you have a bunch of fruits.\n",
      "\n",
      "*   **Traditional Database:** You might store them by name (e.g., \"Apple\", \"Banana\", \"Orange\").\n",
      "*   **Vector Database:** You could represent each fruit by its sweetness, sourness, and color (e.g., Apple: [0.8, 0.2, 0.5], Banana: [0.9, 0.1, 0.3], Orange: [0.7, 0.6, 0.9]).  If you want to find fruits similar to a new fruit with sweetness 0.85, sourness 0.15, and color 0.35, the vector database would quickly find the Banana because its vector is closest.\n",
      "\n",
      "**In summary:**\n",
      "\n",
      "*   **Vectors:**  Number lists that represent the characteristics of something.\n",
      "*   **Vector Database:**  A database optimized for storing and searching vectors based on similarity.\n",
      "\n",
      "**Why are Vector Databases useful?**\n",
      "\n",
      "They're great for:\n",
      "\n",
      "*   **Image Search:** Finding similar images.\n",
      "*   **Recommendation Systems:** Recommending products or content based on what you've liked before.\n",
      "*   **Natural Language Processing (NLP):**  Finding similar sentences or documents.\n",
      "*   **Fraud Detection:** Identifying suspicious transactions that are similar to known fraudulent ones.\n",
      "\n",
      "So, a vector database is a powerful tool for finding things that are *similar* to each other, even if they don't have the same name or exact properties. It does this by representing everything as a list of numbers (a vector) and then comparing those numbers.\n"
     ]
    }
   ],
   "source": [
    "# Build a chain: Prompt â†’ LLM\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Components:\n",
    "# 1. prompt: Formats the input\n",
    "# 2. llm: Generates the response\n",
    "# 3. StrOutputParser: Extracts just the text from the response\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Use the chain with different topics\n",
    "topics = [\"machine learning\", \"embeddings\", \"vector databases\"]\n",
    "\n",
    "for topic in topics:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Topic: {topic.upper()}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Invoke the chain\n",
    "    explanation = chain.invoke({\"topic\": topic})\n",
    "    print(explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“ INTERMEDIATE: Batch Processing\n",
    "\n",
    "Process multiple inputs efficiently using `.batch()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. RAG:\n",
      "   Okay, imagine you have a super smart AI friend who can answer almost any question. This friend is li...\n",
      "\n",
      "2. LCEL:\n",
      "   Okay, let's break down LCEL (LangChain Expression Language) in a way that's easy to understand, even...\n",
      "\n",
      "3. LANGCHAIN AGENTS:\n",
      "   Okay, imagine you have a super smart AI assistant (like ChatGPT) that can do a lot of things, but it...\n"
     ]
    }
   ],
   "source": [
    "# Batch process multiple topics at once\n",
    "topics_batch = [\n",
    "    {\"topic\": \"RAG\"},\n",
    "    {\"topic\": \"LCEL\"},\n",
    "    {\"topic\": \"LangChain agents\"}\n",
    "]\n",
    "\n",
    "# Batch processing is more efficient than calling invoke() multiple times\n",
    "results = chain.batch(topics_batch)\n",
    "\n",
    "# Print results\n",
    "for i, (input_dict, result) in enumerate(zip(topics_batch, results), 1):\n",
    "    print(f\"\\n{i}. {input_dict['topic'].upper()}:\")\n",
    "    print(f\"   {result[:100]}...\")  # Print first 100 chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"comparison\"></a>\n",
    "## 6. LangChain vs Traditional ML Pipelines ğŸ†š\n",
    "\n",
    "### ğŸ”° BEGINNER: Key Differences\n",
    "\n",
    "| Aspect | Traditional ML | LangChain |\n",
    "|--------|---------------|------------|\n",
    "| **Setup** | Complex, manual | Pre-built components |\n",
    "| **Integration** | Write custom code | Use existing integrations |\n",
    "| **Composability** | Difficult to chain | Easy with LCEL |\n",
    "| **Debugging** | Manual logging | Built-in callbacks |\n",
    "| **Prototyping** | Slow | Very fast |\n",
    "| **Code Reuse** | Limited | High |\n",
    "\n",
    "### Example: Building a Q&A System\n",
    "\n",
    "#### Without LangChain (50+ lines):\n",
    "```python\n",
    "import openai\n",
    "import PyPDF2\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load PDF manually\n",
    "def load_pdf(file_path):\n",
    "    reader = PyPDF2.PdfReader(file_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# 2. Split text manually\n",
    "def split_text(text, chunk_size=1000):\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), chunk_size):\n",
    "        chunks.append(text[i:i+chunk_size])\n",
    "    return chunks\n",
    "\n",
    "# 3. Create embeddings manually\n",
    "def create_embeddings(chunks):\n",
    "    embeddings = []\n",
    "    for chunk in chunks:\n",
    "        response = openai.Embedding.create(\n",
    "            input=chunk,\n",
    "            model=\"text-embedding-ada-002\"\n",
    "        )\n",
    "        embeddings.append(response['data'][0]['embedding'])\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# 4. Create vector store manually\n",
    "def create_vector_store(embeddings):\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "\n",
    "# 5. Search manually\n",
    "def search(query, index, chunks):\n",
    "    query_embedding = openai.Embedding.create(\n",
    "        input=query,\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    )['data'][0]['embedding']\n",
    "    \n",
    "    distances, indices = index.search(\n",
    "        np.array([query_embedding]), k=3\n",
    "    )\n",
    "    return [chunks[i] for i in indices[0]]\n",
    "\n",
    "# 6. Generate answer manually\n",
    "def generate_answer(query, context):\n",
    "    prompt = f\"Context: {context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "# Use it:\n",
    "text = load_pdf(\"document.pdf\")\n",
    "chunks = split_text(text)\n",
    "embeddings = create_embeddings(chunks)\n",
    "index = create_vector_store(embeddings)\n",
    "relevant_chunks = search(\"What is RAG?\", index, chunks)\n",
    "answer = generate_answer(\"What is RAG?\", \" \".join(relevant_chunks))\n",
    "```\n",
    "\n",
    "#### With LangChain (10 lines!):\n",
    "```python\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Load, split, embed, and store\n",
    "docs = PyPDFLoader(\"document.pdf\").load()\n",
    "chunks = RecursiveCharacterTextSplitter(chunk_size=1000).split_documents(docs)\n",
    "vectorstore = FAISS.from_documents(chunks, OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Build RAG chain\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    ")\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt | ChatOpenAI() | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Use it:\n",
    "answer = chain.invoke(\"What is RAG?\")\n",
    "```\n",
    "\n",
    "**50+ lines â†’ 10 lines!** ğŸ‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "## 7. Summary & Next Steps ğŸ“\n",
    "\n",
    "### ğŸ‰ What You Learned\n",
    "\n",
    "âœ… **LangChain** is a framework that simplifies building LLM applications\n",
    "\n",
    "âœ… **Architecture** is modular: loaders, splitters, embeddings, vector stores, chains\n",
    "\n",
    "âœ… **Documents** contain `page_content` (text) and `metadata` (information about the text)\n",
    "\n",
    "âœ… **LCEL** uses the `|` operator to chain components together\n",
    "\n",
    "âœ… **Chains** connect multiple components to create complex workflows\n",
    "\n",
    "âœ… LangChain **dramatically reduces code** compared to manual implementations\n",
    "\n",
    "### ğŸ”° For Beginners\n",
    "You now understand:\n",
    "- What LangChain is and why it's useful\n",
    "- How to set up your environment\n",
    "- Basic concepts: Documents and Chains\n",
    "- How to make your first LLM call\n",
    "\n",
    "### ğŸ“ For Intermediate Learners\n",
    "You now understand:\n",
    "- LangChain's package structure (1.0+ reorganization)\n",
    "- LCEL internals and advantages\n",
    "- Metadata usage for filtering and citation\n",
    "- Batch processing for efficiency\n",
    "\n",
    "### ğŸ“š Next Notebooks\n",
    "\n",
    "1. **Notebook 02**: Document Loaders (PDF, CSV, JSON, HTML)\n",
    "2. **Notebook 03**: Text Splitting Strategies\n",
    "3. **Notebook 04**: Embeddings and Vector Representations\n",
    "4. **Notebook 05**: Vector Stores (FAISS, Chroma)\n",
    "5. **Notebook 06**: Retrieval Strategies\n",
    "6. **Notebook 07**: Complete RAG Pipeline\n",
    "\n",
    "### ğŸ’¡ Practice Exercises\n",
    "\n",
    "Before moving to the next notebook, try these:\n",
    "\n",
    "1. **Easy**: Create 5 Documents about different topics with meaningful metadata\n",
    "2. **Medium**: Build a chain that takes a topic and generates a haiku about it\n",
    "3. **Advanced**: Create a chain that summarizes text in different styles (formal, casual, technical)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ Additional Resources\n",
    "\n",
    "- [Official LangChain Documentation](https://python.langchain.com/docs/)\n",
    "- [LCEL Guide](https://python.langchain.com/docs/expression_language/)\n",
    "- [LangChain GitHub](https://github.com/langchain-ai/langchain)\n",
    "\n",
    "---\n",
    "\n",
    "**Ready for more? Continue to Notebook 02: Document Loaders! ğŸš€**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
